{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce5db1c3-f27c-4dcb-8420-37dfc7ad9a61",
   "metadata": {},
   "source": [
    "# Imports and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dad1d9-3e10-44bf-a46e-a4953ba0f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq\n",
    "!pip install mediapipe\n",
    "!pip install FER\n",
    "# Download the shape_predictor_68_face_landmarks.dat file\n",
    "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "!bunzip2 shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd4d96-91d5-4cf3-9eca-64c949ee8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "from fer import FER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d39fc-5df2-4acd-9077-1f80d972ccae",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2200fd58-b9fd-47f9-93ba-73b52c2ed9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = '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'\n",
    "client = Groq(api_key = groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bc9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {\n",
    "    'content': {\n",
    "        'total_questions': 0,\n",
    "        'ans_validation_score': 0,\n",
    "        'quality':{\n",
    "            'unity': 0,\n",
    "            'coherence': 0,\n",
    "            'grammar' : 0,\n",
    "            'optimal_repetetion' : 0,\n",
    "            'clarity' : 0,\n",
    "            'engagement': 0,\n",
    "            'tone_and_style': 0,\n",
    "        },\n",
    "    'audio': {\n",
    "        'fluency' : 0,\n",
    "        'tone_and_clarity' : 0,\n",
    "    },\n",
    "    'video' : {\n",
    "        'eye_contact' : 0,\n",
    "        'expressions' : 0,\n",
    "    }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30f199c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'total_questions': 0,\n",
       "  'ans_validation_score': 0,\n",
       "  'quality': {'unity': 0,\n",
       "   'coherence': 0,\n",
       "   'grammar': 0,\n",
       "   'optimal_repetetion': 0,\n",
       "   'clarity': 0,\n",
       "   'engagement': 0,\n",
       "   'tone_and_style': 0},\n",
       "  'audio': {'fluency': 0, 'tone_and_clarity': 0},\n",
       "  'video': {'eye_contact': 0, 'expressions': 0}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980197b",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c62015be-fb7e-4d6a-8b45-cc714bda7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_from_audio(filename):\n",
    "    with open(filename, \"rb\") as file:\n",
    "        translation = client.audio.translations.create(\n",
    "          file=(filename, file.read()),\n",
    "          model=\"whisper-large-v3\",\n",
    "          # prompt=\"Transcribe the following audio exactly as spoken, including all filler words, repetitions, stammering, and disfluencies. Do not correct grammar, smooth out phrases, or remove any hesitations. The transcription should faithfully represent the speaker's words as they were spoken.\",  # Optional\n",
    "          response_format=\"json\",  # Optional\n",
    "          temperature=0.0  # Optional\n",
    "        )\n",
    "    return translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4640afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_audio_from_video(video_path):\n",
    "    #goutam\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b98a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_from_response(response):\n",
    "    json = ''\n",
    "    return json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d22e6d3",
   "metadata": {},
   "source": [
    "## content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42a848f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversation_from_audio(audio):\n",
    "    # goutam \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2c08233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_quality_scores(text_body):\n",
    "    groq_input = get_groq_input(text_body)\n",
    "    response_content_quality = get_groq_response(groq_input)\n",
    "    json_response_content_quality = get_json_from_response(response_content_quality)\n",
    "    return json_response_content_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5724185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similiarity_score(context, text1, text2):\n",
    "    # goutam\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e29673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_ans_valid_score(qsn_ans_labelled):\n",
    "    '''\n",
    "    question_answers_with_ground_truth format --\n",
    "    {\n",
    "      '001': {\n",
    "        'question':  ...,\n",
    "        'ans_by_candidate' : ...,\n",
    "        'ans_actual' :  ...,\n",
    "        'score': ...,\n",
    "    },\n",
    "    ...\n",
    "    }\n",
    "    '''\n",
    "    qsns = qsn_ans_labelled.keys() \n",
    "    for qsn in qsns:\n",
    "        question = qsn_ans_labelled[qsn]['question']\n",
    "        ans_by_candidate = qsn_ans_labelled[qsn]['ans_by_candidate']\n",
    "        ans_actual = qsn_ans_labelled[qsn]['ans_actual']\n",
    "        qsn_ans_labelled[qsn]['score'] = get_similiarity_score(question, ans_by_candidate, ans_actual)\n",
    "    \n",
    "    total_qsns = len(qsns)\n",
    "    overall_score_ans_validation = [qsn_ans_labelled[qsn]['score'] for qsn in qsns]/total_qsns\n",
    "\n",
    "    return total_qsns, overall_score_ans_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd34cd40",
   "metadata": {},
   "source": [
    "## audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc9b1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_fluency_score(conversation):\n",
    "    #tej\n",
    "    # speech speed consistency\n",
    "\n",
    "    # filler words - 'uh'\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "780876c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_tone_and_clarity_score(audio):\n",
    "    #tej\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba5c6a",
   "metadata": {},
   "source": [
    "## video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed292cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_expresion_for_image(image):\n",
    "#     return dominant_expression, expression_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882fa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_random_n_images(video_path, n):\n",
    "    # fetch n random frames\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    random_frames = sorted(random.sample(range(total_frames), n))\n",
    "    frame_number = 0\n",
    "    while True:\n",
    "        sucess, frame = cap.read()\n",
    "        if not sucess:\n",
    "            break\n",
    "        frame_number+=1\n",
    "        frame_name = str(frame_number).zfill(6)\n",
    "        if frame_number in random_frames:\n",
    "            cv2.imwrite(f'{output_folder}/{filename}/{frame_name}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ce7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_detectors():\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "    fer_detector = FER()\n",
    "    return detector, predictor, fer_detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eyeball_movement(landmarks):\n",
    "    left_eye = landmarks[42:48]\n",
    "    right_eye = landmarks[36:42]\n",
    "    left_eye_center = left_eye.mean(axis=0).astype(\"int\")\n",
    "    right_eye_center = right_eye.mean(axis=0).astype(\"int\")\n",
    "    left_ear = eye_aspect_ratio(left_eye)\n",
    "    right_ear = eye_aspect_ratio(right_eye)\n",
    "    ear = (left_ear + right_ear) / 2.0\n",
    "    return left_eye_center, right_eye_center, ear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a2e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_analysis(video_path, save_video = False):\n",
    "    # Initialize detectors\n",
    "    detector, predictor, fer_detector = initialize_detectors()\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video details\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    if save_video:\n",
    "        # Define the codec and create a VideoWriter object to save the output video\n",
    "        output_path = video_path.split('.')[0] + '_output.mp4'\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Initialize variables\n",
    "    movement_counts = []\n",
    "    blink_counts = 0\n",
    "    emotion_counts = {'angry': 0, 'disgust': 0, 'fear': 0, 'happy': 0, 'sad': 0, 'surprise': 0, 'neutral': 0}\n",
    "    prev_left_eye_center = None\n",
    "    prev_right_eye_center = None\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        for rect in rects:\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            left_eye_center, right_eye_center, ear = calculate_eyeball_movement(shape)\n",
    "\n",
    "            if prev_left_eye_center is not None and prev_right_eye_center is not None:\n",
    "                left_movement = dist.euclidean(prev_left_eye_center, left_eye_center)\n",
    "                right_movement = dist.euclidean(prev_right_eye_center, right_eye_center)\n",
    "                movement_counts.append(left_movement + right_movement)\n",
    "\n",
    "                if ear < 0.21:  # Threshold for blinking\n",
    "                    blink_counts += 1\n",
    "\n",
    "                # Draw eye centers and EAR on the frame\n",
    "                cv2.circle(frame, tuple(left_eye_center), 2, (0, 255, 0), -1)\n",
    "                cv2.circle(frame, tuple(right_eye_center), 2, (0, 255, 0), -1)\n",
    "                cv2.putText(frame, f\"EAR: {ear:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            prev_left_eye_center = left_eye_center\n",
    "            prev_right_eye_center = right_eye_center\n",
    "\n",
    "        # Emotion detection\n",
    "        result = fer_detector.detect_emotions(frame)\n",
    "        for face in result:\n",
    "            emotions = face['emotions']\n",
    "            top_emotion = max(emotions, key=emotions.get)\n",
    "            emotion_counts[top_emotion] += 1\n",
    "            cv2.putText(frame, f\"Emotion: {top_emotion}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "        # Calculate average movement and blink count\n",
    "        avg_movement_rate = np.mean(movement_counts) if movement_counts else 0\n",
    "\n",
    "        # Evaluate the expression-based description\n",
    "        total_frames = sum(emotion_counts.values())\n",
    "        emotion_percentages = {emotion: (count / total_frames) * 100 for emotion, count in emotion_counts.items()} if total_frames > 0 else {emotion: 0 for emotion in emotion_counts}\n",
    "\n",
    "        dominant_emotion = max(emotion_percentages, key=emotion_percentages.get)\n",
    "        dominant_emotion_percentage = emotion_percentages[dominant_emotion]\n",
    "\n",
    "        # Generate detailed description\n",
    "        description = \"\"\n",
    "        confidence_score = 8  # Default confidence score\n",
    "\n",
    "        if dominant_emotion in ['fear', 'disgust']:\n",
    "            description = (\n",
    "                f\"The candidate appears quite uneasy with a dominant emotional state of {dominant_emotion} \"\n",
    "                f\"at {dominant_emotion_percentage:.2f}%. Additionally, there is some evidence of nervousness \"\n",
    "                f\"indicated by eye movements and blinking. Overall, the candidate's demeanor suggests a higher \"\n",
    "                f\"level of anxiety.\"\n",
    "            )\n",
    "            confidence_score -= 2\n",
    "        elif dominant_emotion in ['happy', 'neutral']:\n",
    "            description = (\n",
    "                f\"The candidate seems relaxed or positive with a dominant emotional state of {dominant_emotion} \"\n",
    "                f\"at {dominant_emotion_percentage:.2f}%. The eye movement and blinking are within normal ranges, \"\n",
    "                f\"indicating that the candidate is calm and composed throughout the video.\"\n",
    "            )\n",
    "            confidence_score += 1\n",
    "        else:\n",
    "            description = (\n",
    "                f\"The candidate shows a mixed emotional state with no dominant emotion. Eye movement and blink rate \"\n",
    "                f\"are moderate, suggesting that while the candidate might not be extremely anxious, there are \"\n",
    "                f\"slight signs of nervousness. The overall demeanor is neutral.\"\n",
    "            )\n",
    "\n",
    "        if avg_movement_rate > 2 or blink_counts > 20:\n",
    "            description += (\n",
    "                \" The eye movement and blink count suggest some level of nervousness, adding to the overall \"\n",
    "                \"impression of anxiety or unease.\"\n",
    "            )\n",
    "            confidence_score -= 2\n",
    "\n",
    "        # Ensure confidence score is within 0-10 range\n",
    "        confidence_score = max(0, min(10, confidence_score))\n",
    "\n",
    "        # Display calculated parameters\n",
    "        cv2.putText(frame, f\"Avg Movement Rate: {avg_movement_rate:.2f}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Blink Count: {blink_counts}\", (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Dominant Emotion: {dominant_emotion.capitalize()} ({dominant_emotion_percentage:.2f}%)\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Overall Description: {description}\", (10, 180), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(frame, f\"Confidence Score: {confidence_score}/10\", (10, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        if save_video:\n",
    "            out.write(frame)\n",
    "\n",
    "    # Release the video capture and writer objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Output results\n",
    "    print(f\"Output video saved to {output_path}\")\n",
    "    print(f\"Average Eye Movement Rate: {avg_movement_rate:.2f}\")\n",
    "    print(f\"Blink Count: {blink_counts}\")\n",
    "    print(f\"Emotion Counts: {emotion_counts}\")\n",
    "    print(f\"Overall Description: {description}\")\n",
    "    print(f\"Confidence Score: {confidence_score}/10\")\n",
    "\n",
    "    return avg_movement_rate, blink_counts, emotion_counts, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec1eef50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m all_scores\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_scores' is not defined"
     ]
    }
   ],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7342248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_scores():\n",
    "    all_scores['content']['quality'] = get_content_quality_scores()\n",
    "    all_scores['content']['quality'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf46aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f94fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = ''\n",
    "output_folder = 'output'\n",
    "filename = video_path.split('/')[-1].split('.')[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
